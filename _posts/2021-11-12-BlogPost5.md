---
layout: post
title: Blog Post 5
---
## Blog Post 5
In this blog post we will be working in TENSORFLOW! Tensorflow and keras are packages in python that help us create neural networks! For this blog post we are going to see if we can train a neural network to distinguish between images of cats and images of dogs! 

## Part 1: Load Packages and Obtain Data
We will be running the following code blocks to get our data from a google API. 


```python
#importing packages 
import os
from tensorflow.keras import utils, layers, models 
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
```


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.
    


```python
#We are going to store the class names before running the code block below in order to store the different class_names
class_names = train_dataset.class_names
class_names #let's check out our labels
```




    ['cats', 'dogs']




```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

## two_rowvisual() function
Let's see some examples of the images we may be working with by running the following function! This function gets images from one batch of our training data (32 images) and examines the label. If it is labeled as a cat we put it in a list of cat_images and if it is a dog we put it in a list of dog_images. We then plot the images in a 2 row x 3 column grid. 


```python
def two_rowvisual():
  plt.figure(figsize=(10, 10))
  #organize images into two different lists
  for images, labels in train_dataset.take(1):
    cat_images = []
    dog_images = []
    while len(cat_images) <= 3 & len(dog_images) <=3:
      for i in range(32): #go through batch of 32 images
        if class_names[labels[i]] == 'dogs': #if label == dog, put in dog list
          dog_images.append(images[i]) #if label == cat, put in cat list 
        else:
          cat_images.append(images[i])
    fig,ax = plt.subplots(2, 3)
    for i in range(3):#plot the images in two seperate rows
      #cat images go in top row, dogs in bottom   
      ax[0,i].imshow(cat_images[i].numpy().astype("uint8"))
      ax[0,i].axis('off')
      ax[0,i].set_title('Cat')
      ax[1,i].set_title('Dog')
      ax[1,i].imshow(dog_images[i].numpy().astype("uint8"))
      ax[1,i].axis('off')
      #print(dog_images[i].shape) #double check shapes of our images
        
      
```

Let's now run it! 


```python
two_rowvisual()
```


    <Figure size 720x720 with 0 Axes>



    
![images/catsanddogs_PNG](/images/cats_dogs.PNG)
    


## Checking Label Frequencies 
We are going to create an iterator that goes through the labels of our datasets. Since we know that there are 2000 images, we will use the labels_iterator for 2000 images and get the counts of each label. If it is a 0 it is a cat image and if it is a 1 it is a dog image.


```python
#Create an interator for our labels
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
cat_count = 0
dog_count = 0;
for i in range(0,2000): #we know that there are 2000 training images
  if next(labels_iterator) == 0:
    cat_count +=1
  else:
    dog_count +=1

cat_count,dog_count
```




    (1000, 1000)



As we can see, the ratio is split 50-50 between cat images and dog images. If we were to implement a benchmark model that predicts based on which animal is more frequent in the dataset, it would predict either cats or dogs 100% of the time with 50% accuracy. This is not ideal!

## Part 2: First Model
We are going to create our first Tensorflow model! Our first model will be comprised of Conv2D layers, MaxPooling2D layers, and dropout layers. Conv2D layers train our kernels and apply a recitified linear unit function to our output, MaxPooling 2D layers take the highest values in our tensor in a 2x2 grid, while dropout layers are good to reduce overfitting by "dropping" random values! 


```python
model1 = models.Sequential([
                            layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (160,160,3)),
                            layers.MaxPooling2D((2,2)),
                            layers.Dropout(0.2),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Dropout(0.2),
                            layers.Flatten(),
                            layers.Dense(2)

])
model1.compile(optimizer = 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```

Let's see how our model is summarized! The following code allows us to check parameters, and output shapes of each layer


```python
model1.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d (Conv2D)             (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 79, 79, 32)       0         
     )                                                               
                                                                     
     dropout (Dropout)           (None, 79, 79, 32)        0         
                                                                     
     conv2d_1 (Conv2D)           (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 38, 38, 32)       0         
     2D)                                                             
                                                                     
     dropout_1 (Dropout)         (None, 38, 38, 32)        0         
                                                                     
     flatten (Flatten)           (None, 46208)             0         
                                                                     
     dense (Dense)               (None, 2)                 92418     
                                                                     
    =================================================================
    Total params: 102,562
    Trainable params: 102,562
    Non-trainable params: 0
    _________________________________________________________________
    

Cool! Now let's see our training history of our model on our training data and the validation accuracy of the validation data!


```python
history = model1.fit(train_dataset,epochs = 20, validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 80ms/step - loss: 34.2808 - accuracy: 0.4870 - val_loss: 0.6919 - val_accuracy: 0.5198
    Epoch 2/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.6927 - accuracy: 0.5305 - val_loss: 0.6957 - val_accuracy: 0.5161
    Epoch 3/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.6748 - accuracy: 0.5605 - val_loss: 0.7153 - val_accuracy: 0.5396
    Epoch 4/20
    63/63 [==============================] - 5s 73ms/step - loss: 0.6625 - accuracy: 0.5655 - val_loss: 0.7401 - val_accuracy: 0.5334
    Epoch 5/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.6277 - accuracy: 0.6005 - val_loss: 0.7569 - val_accuracy: 0.5396
    Epoch 6/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5998 - accuracy: 0.6185 - val_loss: 0.7842 - val_accuracy: 0.5408
    Epoch 7/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5743 - accuracy: 0.6360 - val_loss: 0.7891 - val_accuracy: 0.5297
    Epoch 8/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.5472 - accuracy: 0.6595 - val_loss: 0.8450 - val_accuracy: 0.5458
    Epoch 9/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.5111 - accuracy: 0.6855 - val_loss: 0.9902 - val_accuracy: 0.5173
    Epoch 10/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.4876 - accuracy: 0.7100 - val_loss: 1.0530 - val_accuracy: 0.5124
    Epoch 11/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.4678 - accuracy: 0.7285 - val_loss: 1.1239 - val_accuracy: 0.5173
    Epoch 12/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.4333 - accuracy: 0.7665 - val_loss: 1.0955 - val_accuracy: 0.5495
    Epoch 13/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.4396 - accuracy: 0.7635 - val_loss: 1.5604 - val_accuracy: 0.5248
    Epoch 14/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.4291 - accuracy: 0.7770 - val_loss: 1.2224 - val_accuracy: 0.5309
    Epoch 15/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.3777 - accuracy: 0.8085 - val_loss: 1.3938 - val_accuracy: 0.5421
    Epoch 16/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.3797 - accuracy: 0.8110 - val_loss: 1.3948 - val_accuracy: 0.5470
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.3719 - accuracy: 0.8300 - val_loss: 1.3697 - val_accuracy: 0.5235
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.3404 - accuracy: 0.8420 - val_loss: 1.7276 - val_accuracy: 0.5347
    Epoch 19/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.3208 - accuracy: 0.8530 - val_loss: 1.6716 - val_accuracy: 0.5644
    Epoch 20/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.3133 - accuracy: 0.8555 - val_loss: 1.9844 - val_accuracy: 0.5569
    

The validation accuracy of my model stabilized **between 50% and 55%** during training. Compared to the baseline this is slightly better! However, there is definitely overfitting going on here, as the validation accuracy is much lower than the training accuracy (stabilized between 80 and 85%). 

Let's try altering some layers to see if we can improve our validation accuracy!


```python
model1_2 = models.Sequential([
                            layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (160,160,3)),
                            layers.MaxPooling2D((2,2)),
                            layers.Dropout(0.2),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Flatten(),
                            layers.Dense(2)

])
model1_2.compile(optimizer = 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```


```python
model1_2.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d_2 (Conv2D)           (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_2 (MaxPooling  (None, 79, 79, 32)       0         
     2D)                                                             
                                                                     
     dropout_2 (Dropout)         (None, 79, 79, 32)        0         
                                                                     
     conv2d_3 (Conv2D)           (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_3 (MaxPooling  (None, 38, 38, 32)       0         
     2D)                                                             
                                                                     
     conv2d_4 (Conv2D)           (None, 36, 36, 32)        9248      
                                                                     
     max_pooling2d_4 (MaxPooling  (None, 18, 18, 32)       0         
     2D)                                                             
                                                                     
     flatten_1 (Flatten)         (None, 10368)             0         
                                                                     
     dense_1 (Dense)             (None, 2)                 20738     
                                                                     
    =================================================================
    Total params: 40,130
    Trainable params: 40,130
    Non-trainable params: 0
    _________________________________________________________________
    


```python
history1_2 = model1_2.fit(train_dataset,epochs = 20, validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 82ms/step - loss: 11.0025 - accuracy: 0.4950 - val_loss: 0.6957 - val_accuracy: 0.4864
    Epoch 2/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6946 - accuracy: 0.5000 - val_loss: 0.6968 - val_accuracy: 0.4864
    Epoch 3/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6906 - accuracy: 0.4915 - val_loss: 0.6984 - val_accuracy: 0.5408
    Epoch 4/20
    63/63 [==============================] - 6s 99ms/step - loss: 0.6848 - accuracy: 0.5155 - val_loss: 0.6973 - val_accuracy: 0.5210
    Epoch 5/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6773 - accuracy: 0.5460 - val_loss: 0.6969 - val_accuracy: 0.5458
    Epoch 6/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6677 - accuracy: 0.5525 - val_loss: 0.7063 - val_accuracy: 0.5470
    Epoch 7/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6698 - accuracy: 0.5565 - val_loss: 0.7138 - val_accuracy: 0.5408
    Epoch 8/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6473 - accuracy: 0.5650 - val_loss: 0.7135 - val_accuracy: 0.5396
    Epoch 9/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6627 - accuracy: 0.5585 - val_loss: 0.7120 - val_accuracy: 0.5421
    Epoch 10/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6548 - accuracy: 0.5705 - val_loss: 0.7260 - val_accuracy: 0.5198
    Epoch 11/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6347 - accuracy: 0.5790 - val_loss: 0.8172 - val_accuracy: 0.5557
    Epoch 12/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6123 - accuracy: 0.6050 - val_loss: 0.8253 - val_accuracy: 0.5668
    Epoch 13/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.6183 - accuracy: 0.6070 - val_loss: 0.7907 - val_accuracy: 0.5235
    Epoch 14/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5981 - accuracy: 0.6155 - val_loss: 0.8235 - val_accuracy: 0.5210
    Epoch 15/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5840 - accuracy: 0.6370 - val_loss: 0.8448 - val_accuracy: 0.5223
    Epoch 16/20
    63/63 [==============================] - 5s 74ms/step - loss: 0.5856 - accuracy: 0.6345 - val_loss: 0.9710 - val_accuracy: 0.5334
    Epoch 17/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.5609 - accuracy: 0.6470 - val_loss: 0.9006 - val_accuracy: 0.5235
    Epoch 18/20
    63/63 [==============================] - 5s 76ms/step - loss: 0.5531 - accuracy: 0.6555 - val_loss: 0.8432 - val_accuracy: 0.5223
    Epoch 19/20
    63/63 [==============================] - 5s 75ms/step - loss: 0.5437 - accuracy: 0.6825 - val_loss: 1.0818 - val_accuracy: 0.5161
    Epoch 20/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5280 - accuracy: 0.6700 - val_loss: 0.9612 - val_accuracy: 0.5037
    

Affter removing one dropout layer and adding instead another conv2D and MaxPooling2D layer, we can see that our validation accuracy is more consistently passed 52% and **between 53 and 57%!** We still see some overfitting compared to the training data, but compared to the baseline of 50% this is much better! 

## Part 3: Model with Data Augmentation
We can actually increase the robustness of our model by training it on images from our training dataset that have been altered! By randomizing features that should not factor into identification of dogs and cats such as rotation angle or orientation, we can train our neural network to no longer rely on factors such as that! 


```python
##Creation of Data Augmentation Layers
random_flip = layers.experimental.preprocessing.RandomFlip(
    #mode specifies which directions we want to randomly flip our image
    mode="horizontal_and_vertical", name='RandomFlip',seed = None 
)


```

Let's make sure our layer is doing what we want it to! The following code applies our layer to a random image taken from a random batch in our training data!


```python
#Plotting the original image and the flipped image
for images, labels in train_dataset.take(1):
  fig,ax = plt.subplots(1,4, figsize = (20,20)) #create plot
  ax[0].imshow(images[0].numpy().astype("uint8")) #plot original image
  ax[0].set_title('Original')
  test = random_flip(images[0].numpy().astype('uint8'))  #plot random flipped image
  ax[1].imshow(test)
  ax[1].set_title('Random Flip')
  test1 = random_flip(images[0].numpy().astype("uint8")) #plot again randomly flipped 
  ax[2].imshow(test1)
  ax[2].set_title('Random Flip')
  test2 = random_flip(images[0].numpy().astype("uint8"))
  ax[3].imshow(test2)
  ax[3].set_title('Random Flip')
```


    
![rand_flipPNG](/images/rand_flip.PNG)
    


Now let's create another augmentation layer, this time it will rotate randomly in the range -2pi to 2pi! 


```python
## Creation of a Data Augmentation Layer: RandomRotation
random_rotation = layers.RandomRotation(
    #factor specifies the range in which our image will be rotated multiplied by 2pi!
    factor = (-1,1), fill_mode = "reflect", seed = None
)
```

Again! let's see how our layer does! 


```python
for images, labels in train_dataset.take(1):
  fig,ax = plt.subplots(1,4, figsize = (20,20))
  ax[0].imshow(images[0].numpy().astype("uint8"))
  ax[0].set_title('Original')
  test = random_rotation(images[0].numpy().astype('uint8')) 
  ax[1].imshow(test)
  ax[1].set_title('Random Rotation')
  test1 = random_rotation(images[0].numpy().astype("uint8"))
  ax[2].imshow(test1)
  ax[2].set_title('Random Rotation')
  test2 = random_rotation(images[0].numpy().astype("uint8"))
  ax[3].imshow(test2)
  ax[3].set_title('Random Rotation')
```


    
![rand_rotation_PNG](/images/rand_rotation.PNG)
    


Great! Now we can add these layers to our model!!!! 


```python
model2 = models.Sequential([
                            random_flip,
                            random_rotation,
                            layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (160,160,3)),
                            layers.MaxPooling2D((2,2)),
                            layers.Dropout(0.2),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Flatten(),
                            layers.Dense(2)

])
model2.compile(optimizer = 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```


```python
history2 = model2.fit(train_dataset,epochs = 20, validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 81ms/step - loss: 5.4478 - accuracy: 0.5095 - val_loss: 0.6996 - val_accuracy: 0.4963
    Epoch 2/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6938 - accuracy: 0.5170 - val_loss: 0.7020 - val_accuracy: 0.5012
    Epoch 3/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6920 - accuracy: 0.5195 - val_loss: 0.7024 - val_accuracy: 0.5000
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6954 - accuracy: 0.5040 - val_loss: 0.6947 - val_accuracy: 0.5136
    Epoch 5/20
    63/63 [==============================] - 6s 98ms/step - loss: 0.6927 - accuracy: 0.5095 - val_loss: 0.6983 - val_accuracy: 0.5062
    Epoch 6/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6925 - accuracy: 0.5120 - val_loss: 0.6951 - val_accuracy: 0.5421
    Epoch 7/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6940 - accuracy: 0.5085 - val_loss: 0.6930 - val_accuracy: 0.5334
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6927 - accuracy: 0.5180 - val_loss: 0.6970 - val_accuracy: 0.5099
    Epoch 9/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6924 - accuracy: 0.5020 - val_loss: 0.6963 - val_accuracy: 0.5223
    Epoch 10/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6919 - accuracy: 0.5060 - val_loss: 0.6940 - val_accuracy: 0.5322
    Epoch 11/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6918 - accuracy: 0.5220 - val_loss: 0.6955 - val_accuracy: 0.5309
    Epoch 12/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6941 - accuracy: 0.5110 - val_loss: 0.6934 - val_accuracy: 0.5173
    Epoch 13/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6938 - accuracy: 0.5235 - val_loss: 0.6959 - val_accuracy: 0.5198
    Epoch 14/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6939 - accuracy: 0.5175 - val_loss: 0.6952 - val_accuracy: 0.5099
    Epoch 15/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6927 - accuracy: 0.5010 - val_loss: 0.6940 - val_accuracy: 0.5186
    Epoch 16/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6929 - accuracy: 0.5265 - val_loss: 0.6957 - val_accuracy: 0.5149
    Epoch 17/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6924 - accuracy: 0.5240 - val_loss: 0.6938 - val_accuracy: 0.5408
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6931 - accuracy: 0.5100 - val_loss: 0.6935 - val_accuracy: 0.5507
    Epoch 19/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6928 - accuracy: 0.5325 - val_loss: 0.6894 - val_accuracy: 0.5718
    Epoch 20/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6919 - accuracy: 0.5125 - val_loss: 0.6927 - val_accuracy: 0.5384
    


```python
model2.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     RandomFlip (RandomFlip)     (None, 160, 160, 3)       0         
                                                                     
     random_rotation (RandomRota  (None, 160, 160, 3)      0         
     tion)                                                           
                                                                     
     conv2d_5 (Conv2D)           (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_5 (MaxPooling  (None, 79, 79, 32)       0         
     2D)                                                             
                                                                     
     dropout_3 (Dropout)         (None, 79, 79, 32)        0         
                                                                     
     conv2d_6 (Conv2D)           (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_6 (MaxPooling  (None, 38, 38, 32)       0         
     2D)                                                             
                                                                     
     conv2d_7 (Conv2D)           (None, 36, 36, 32)        9248      
                                                                     
     max_pooling2d_7 (MaxPooling  (None, 18, 18, 32)       0         
     2D)                                                             
                                                                     
     flatten_2 (Flatten)         (None, 10368)             0         
                                                                     
     dense_2 (Dense)             (None, 2)                 20738     
                                                                     
    =================================================================
    Total params: 40,130
    Trainable params: 40,130
    Non-trainable params: 0
    _________________________________________________________________
    

Yikes! This does not look great! Perhaps we should try altering some layers to see if we can get our model's accuracy up! 


```python
model2_1 = models.Sequential([
                            random_flip,
                            random_rotation,
                            layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (160,160,3)),
                            layers.MaxPooling2D((2,2)),
                            layers.Dropout(0.2),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Flatten(),
                            layers.Dense(2)

])
model2_1.compile(optimizer = 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```


```python
history2_1 = model2_1.fit(train_dataset,epochs = 20, validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 87ms/step - loss: 3.0731 - accuracy: 0.5000 - val_loss: 0.7109 - val_accuracy: 0.5087
    Epoch 2/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.7029 - accuracy: 0.4890 - val_loss: 0.6999 - val_accuracy: 0.4975
    Epoch 3/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6965 - accuracy: 0.4875 - val_loss: 0.6977 - val_accuracy: 0.5025
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6945 - accuracy: 0.4910 - val_loss: 0.6964 - val_accuracy: 0.4715
    Epoch 5/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6962 - val_accuracy: 0.4950
    Epoch 6/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6911 - accuracy: 0.4995 - val_loss: 0.6959 - val_accuracy: 0.5050
    Epoch 7/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6920 - accuracy: 0.5220 - val_loss: 0.7008 - val_accuracy: 0.4691
    Epoch 8/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6942 - accuracy: 0.5035 - val_loss: 0.6953 - val_accuracy: 0.5210
    Epoch 9/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6918 - accuracy: 0.5100 - val_loss: 0.6938 - val_accuracy: 0.5136
    Epoch 10/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6930 - accuracy: 0.5130 - val_loss: 0.6934 - val_accuracy: 0.5136
    Epoch 11/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6934 - accuracy: 0.5155 - val_loss: 0.6933 - val_accuracy: 0.5062
    Epoch 12/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6900 - accuracy: 0.5260 - val_loss: 0.6953 - val_accuracy: 0.5198
    Epoch 13/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6924 - accuracy: 0.5105 - val_loss: 0.6948 - val_accuracy: 0.5223
    Epoch 14/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6924 - accuracy: 0.5230 - val_loss: 0.6950 - val_accuracy: 0.4926
    Epoch 15/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6894 - accuracy: 0.5300 - val_loss: 0.6932 - val_accuracy: 0.5025
    Epoch 16/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6922 - accuracy: 0.5185 - val_loss: 0.6948 - val_accuracy: 0.5285
    Epoch 17/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6934 - accuracy: 0.5175 - val_loss: 0.6964 - val_accuracy: 0.4988
    Epoch 18/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.6942 - accuracy: 0.5095 - val_loss: 0.6936 - val_accuracy: 0.5050
    Epoch 19/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6916 - accuracy: 0.5170 - val_loss: 0.6924 - val_accuracy: 0.5309
    Epoch 20/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6908 - accuracy: 0.5235 - val_loss: 0.6937 - val_accuracy: 0.5210
    


```python
model2_1.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     RandomFlip (RandomFlip)     (None, 160, 160, 3)       0         
                                                                     
     random_rotation (RandomRota  (None, 160, 160, 3)      0         
     tion)                                                           
                                                                     
     conv2d_8 (Conv2D)           (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_8 (MaxPooling  (None, 79, 79, 32)       0         
     2D)                                                             
                                                                     
     dropout_4 (Dropout)         (None, 79, 79, 32)        0         
                                                                     
     conv2d_9 (Conv2D)           (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_9 (MaxPooling  (None, 38, 38, 32)       0         
     2D)                                                             
                                                                     
     conv2d_10 (Conv2D)          (None, 36, 36, 32)        9248      
                                                                     
     max_pooling2d_10 (MaxPoolin  (None, 18, 18, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_11 (Conv2D)          (None, 16, 16, 32)        9248      
                                                                     
     max_pooling2d_11 (MaxPoolin  (None, 8, 8, 32)         0         
     g2D)                                                            
                                                                     
     flatten_3 (Flatten)         (None, 2048)              0         
                                                                     
     dense_3 (Dense)             (None, 2)                 4098      
                                                                     
    =================================================================
    Total params: 32,738
    Trainable params: 32,738
    Non-trainable params: 0
    _________________________________________________________________
    

Now that's more like it! It seems that by adding another set of conv2d/maxpooling layers we were able to provide further processing of our augmented data! Now our validation data is consistently in the **59% - 65%** range! Once again we do see a bit of overfitting but much less than in model1! Not only that but the validation accuracy is consistently much higher than that of model1 as well!

## Part 4: Data Preprocessing
Now we are going to create a layer that transforms the input data by standardizing the RGB values between 0 and 1. Doing so allows for more computational power to be dedicated to learning the weights of the model rather than doing this during training! 


```python
## Data Preprocessing Layer
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

NOw let's toss this layer to the beginning of the model!


```python
model3 = models.Sequential([
                            preprocessor,
                            random_flip,
                            random_rotation,
                            layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (160,160,3)),
                            layers.MaxPooling2D((2,2)),
                            layers.Dropout(0.2),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Flatten(),
                            layers.Dense(2)

])
model3.compile(optimizer = 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```


```python
history3 = model3.fit(train_dataset,epochs = 20, validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 82ms/step - loss: 0.6902 - accuracy: 0.5445 - val_loss: 0.6812 - val_accuracy: 0.5520
    Epoch 2/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6719 - accuracy: 0.5695 - val_loss: 0.6421 - val_accuracy: 0.6324
    Epoch 3/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6465 - accuracy: 0.6255 - val_loss: 0.6463 - val_accuracy: 0.5978
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6263 - accuracy: 0.6415 - val_loss: 0.6344 - val_accuracy: 0.6213
    Epoch 5/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6087 - accuracy: 0.6635 - val_loss: 0.6042 - val_accuracy: 0.6634
    Epoch 6/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5923 - accuracy: 0.6700 - val_loss: 0.5963 - val_accuracy: 0.6621
    Epoch 7/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.5922 - accuracy: 0.6865 - val_loss: 0.5834 - val_accuracy: 0.6745
    Epoch 8/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5776 - accuracy: 0.6870 - val_loss: 0.5670 - val_accuracy: 0.6943
    Epoch 9/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5740 - accuracy: 0.6995 - val_loss: 0.5689 - val_accuracy: 0.6955
    Epoch 10/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5674 - accuracy: 0.6995 - val_loss: 0.5592 - val_accuracy: 0.7017
    Epoch 11/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5695 - accuracy: 0.6965 - val_loss: 0.5884 - val_accuracy: 0.6906
    Epoch 12/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5485 - accuracy: 0.7065 - val_loss: 0.5767 - val_accuracy: 0.6931
    Epoch 13/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.5615 - accuracy: 0.7150 - val_loss: 0.5772 - val_accuracy: 0.6955
    Epoch 14/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.5418 - accuracy: 0.7230 - val_loss: 0.5869 - val_accuracy: 0.6869
    Epoch 15/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.5336 - accuracy: 0.7260 - val_loss: 0.6009 - val_accuracy: 0.6819
    Epoch 16/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5474 - accuracy: 0.7290 - val_loss: 0.5851 - val_accuracy: 0.6807
    Epoch 17/20
    63/63 [==============================] - 6s 82ms/step - loss: 0.5329 - accuracy: 0.7285 - val_loss: 0.5401 - val_accuracy: 0.7104
    Epoch 18/20
    63/63 [==============================] - 5s 82ms/step - loss: 0.5194 - accuracy: 0.7470 - val_loss: 0.5403 - val_accuracy: 0.7178
    Epoch 19/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5203 - accuracy: 0.7350 - val_loss: 0.5261 - val_accuracy: 0.7240
    Epoch 20/20
    63/63 [==============================] - 5s 81ms/step - loss: 0.5082 - accuracy: 0.7560 - val_loss: 0.5631 - val_accuracy: 0.7092
    


```python
model3.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     RandomFlip (RandomFlip)     (None, 160, 160, 3)       0         
                                                                     
     random_rotation (RandomRota  (None, 160, 160, 3)      0         
     tion)                                                           
                                                                     
     conv2d_12 (Conv2D)          (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_12 (MaxPoolin  (None, 79, 79, 32)       0         
     g2D)                                                            
                                                                     
     dropout_5 (Dropout)         (None, 79, 79, 32)        0         
                                                                     
     conv2d_13 (Conv2D)          (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_13 (MaxPoolin  (None, 38, 38, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_14 (Conv2D)          (None, 36, 36, 32)        9248      
                                                                     
     max_pooling2d_14 (MaxPoolin  (None, 18, 18, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_15 (Conv2D)          (None, 16, 16, 32)        9248      
                                                                     
     max_pooling2d_15 (MaxPoolin  (None, 8, 8, 32)         0         
     g2D)                                                            
                                                                     
     flatten_4 (Flatten)         (None, 2048)              0         
                                                                     
     dense_4 (Dense)             (None, 2)                 4098      
                                                                     
    =================================================================
    Total params: 32,738
    Trainable params: 32,738
    Non-trainable params: 0
    _________________________________________________________________
    

Hmmmmm while we are able to surpass 70% on some layers, it is not as consistent as we would like it to be! Let's see if we can't tweak a couple layers to see if we can fix it!



```python
model3_1 = models.Sequential([
                            preprocessor,
                            random_flip,
                            random_rotation,
                            layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (160,160,3)),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Conv2D(32,(3,3), activation = 'relu'),
                            layers.MaxPooling2D((2,2)),
                            layers.Dropout(0.2),
                            layers.Flatten(),
                            layers.Dense(2)

])
model3_1.compile(optimizer = 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```


```python
history3_1 = model3_1.fit(train_dataset,epochs = 20, validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 79ms/step - loss: 0.6941 - accuracy: 0.5215 - val_loss: 0.6716 - val_accuracy: 0.5903
    Epoch 2/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.6512 - accuracy: 0.6115 - val_loss: 0.6068 - val_accuracy: 0.6597
    Epoch 3/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.6287 - accuracy: 0.6445 - val_loss: 0.6064 - val_accuracy: 0.6621
    Epoch 4/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6089 - accuracy: 0.6645 - val_loss: 0.5849 - val_accuracy: 0.6757
    Epoch 5/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.6079 - accuracy: 0.6505 - val_loss: 0.5989 - val_accuracy: 0.6621
    Epoch 6/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5854 - accuracy: 0.6895 - val_loss: 0.6068 - val_accuracy: 0.6460
    Epoch 7/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5805 - accuracy: 0.6935 - val_loss: 0.5884 - val_accuracy: 0.6720
    Epoch 8/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5787 - accuracy: 0.6925 - val_loss: 0.5652 - val_accuracy: 0.6980
    Epoch 9/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5747 - accuracy: 0.6950 - val_loss: 0.5985 - val_accuracy: 0.6621
    Epoch 10/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5668 - accuracy: 0.7080 - val_loss: 0.5572 - val_accuracy: 0.7141
    Epoch 11/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5640 - accuracy: 0.7060 - val_loss: 0.5585 - val_accuracy: 0.7104
    Epoch 12/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5506 - accuracy: 0.7140 - val_loss: 0.5508 - val_accuracy: 0.7104
    Epoch 13/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5512 - accuracy: 0.7150 - val_loss: 0.5377 - val_accuracy: 0.7166
    Epoch 14/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5390 - accuracy: 0.7215 - val_loss: 0.5722 - val_accuracy: 0.6931
    Epoch 15/20
    63/63 [==============================] - 5s 77ms/step - loss: 0.5440 - accuracy: 0.7240 - val_loss: 0.5531 - val_accuracy: 0.7067
    Epoch 16/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5235 - accuracy: 0.7415 - val_loss: 0.5359 - val_accuracy: 0.7240
    Epoch 17/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5414 - accuracy: 0.7155 - val_loss: 0.5374 - val_accuracy: 0.7203
    Epoch 18/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5223 - accuracy: 0.7390 - val_loss: 0.5256 - val_accuracy: 0.7376
    Epoch 19/20
    63/63 [==============================] - 5s 78ms/step - loss: 0.5240 - accuracy: 0.7285 - val_loss: 0.5439 - val_accuracy: 0.7265
    Epoch 20/20
    63/63 [==============================] - 5s 79ms/step - loss: 0.5186 - accuracy: 0.7340 - val_loss: 0.5276 - val_accuracy: 0.7302
    


```python
model3_1.summary()
```

    Model: "sequential_5"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     RandomFlip (RandomFlip)     (None, 160, 160, 3)       0         
                                                                     
     random_rotation (RandomRota  (None, 160, 160, 3)      0         
     tion)                                                           
                                                                     
     conv2d_16 (Conv2D)          (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_16 (MaxPoolin  (None, 79, 79, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_17 (Conv2D)          (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_17 (MaxPoolin  (None, 38, 38, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_18 (Conv2D)          (None, 36, 36, 32)        9248      
                                                                     
     max_pooling2d_18 (MaxPoolin  (None, 18, 18, 32)       0         
     g2D)                                                            
                                                                     
     dropout_6 (Dropout)         (None, 18, 18, 32)        0         
                                                                     
     flatten_5 (Flatten)         (None, 10368)             0         
                                                                     
     dense_5 (Dense)             (None, 2)                 20738     
                                                                     
    =================================================================
    Total params: 40,130
    Trainable params: 40,130
    Non-trainable params: 0
    _________________________________________________________________
    

Wow! Look at that! Our model3_1 was able to maintain a validation accuracy of **70% to 75%**! Interestingly enough, by shifting the dropout layer before the flatten layer and removing a conv2D layer and a maxpooling2D later seemed to do the trick! Perhaps the maxpooling layer's averaging of information is what was costing us some accuracy? There seems to be ver minimal overfitting here, as test data accuracy and validation accuracy was pretty similar throughout. In fact, there are a couple of instances in which the validation accuracy was HIGHER than the test accuracy!

## Part 5: Transfer Learning
You may be wondering what exaclty we are transferring. We are actually going to be transferring another pre-made model called MobileNetV2 into our model as another layer! 

The folllowing code block will create our layer after downloading the model


```python
## Transfer Learning
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model4 = models.Sequential([
                            preprocessor,
                            random_flip,
                            random_rotation,
                            base_model_layer,
                            layers.Dropout(.2),
                            layers.Flatten(),
                            layers.Dense(2)

])
model4.compile(optimizer = 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
```


```python
model4.summary()
```

    Model: "sequential_6"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     RandomFlip (RandomFlip)     (None, 160, 160, 3)       0         
                                                                     
     random_rotation (RandomRota  (None, 160, 160, 3)      0         
     tion)                                                           
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     dropout_7 (Dropout)         (None, 5, 5, 1280)        0         
                                                                     
     flatten_6 (Flatten)         (None, 32000)             0         
                                                                     
     dense_6 (Dense)             (None, 2)                 64002     
                                                                     
    =================================================================
    Total params: 2,321,986
    Trainable params: 64,002
    Non-trainable params: 2,257,984
    _________________________________________________________________
    

Wow...Look at the amount of parameters in the base_model_layer. The amount of parameters being trained in that layer alone is 2257984. For context, the dense layer only has 64002 parameters. Let's look at the training history!


```python
history4 = model4.fit(train_dataset,epochs = 20, validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 11s 112ms/step - loss: 0.8966 - accuracy: 0.8675 - val_loss: 0.2332 - val_accuracy: 0.9567
    Epoch 2/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.8173 - accuracy: 0.8990 - val_loss: 0.2789 - val_accuracy: 0.9517
    Epoch 3/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.7286 - accuracy: 0.9145 - val_loss: 0.2941 - val_accuracy: 0.9691
    Epoch 4/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.6419 - accuracy: 0.9200 - val_loss: 0.2781 - val_accuracy: 0.9517
    Epoch 5/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.5200 - accuracy: 0.9250 - val_loss: 0.2391 - val_accuracy: 0.9703
    Epoch 6/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.7778 - accuracy: 0.9125 - val_loss: 0.3300 - val_accuracy: 0.9629
    Epoch 7/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.6999 - accuracy: 0.9300 - val_loss: 0.3336 - val_accuracy: 0.9604
    Epoch 8/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5390 - accuracy: 0.9385 - val_loss: 0.3443 - val_accuracy: 0.9629
    Epoch 9/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.5541 - accuracy: 0.9480 - val_loss: 0.2515 - val_accuracy: 0.9703
    Epoch 10/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.7084 - accuracy: 0.9270 - val_loss: 0.3861 - val_accuracy: 0.9703
    Epoch 11/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.6724 - accuracy: 0.9395 - val_loss: 0.2569 - val_accuracy: 0.9691
    Epoch 12/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5419 - accuracy: 0.9405 - val_loss: 0.4160 - val_accuracy: 0.9653
    Epoch 13/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.6705 - accuracy: 0.9335 - val_loss: 0.3871 - val_accuracy: 0.9567
    Epoch 14/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5633 - accuracy: 0.9505 - val_loss: 0.2050 - val_accuracy: 0.9777
    Epoch 15/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5005 - accuracy: 0.9440 - val_loss: 0.3843 - val_accuracy: 0.9567
    Epoch 16/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.7901 - accuracy: 0.9365 - val_loss: 0.4912 - val_accuracy: 0.9641
    Epoch 17/20
    63/63 [==============================] - 6s 88ms/step - loss: 0.5580 - accuracy: 0.9475 - val_loss: 0.5165 - val_accuracy: 0.9567
    Epoch 18/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5435 - accuracy: 0.9455 - val_loss: 0.6587 - val_accuracy: 0.9579
    Epoch 19/20
    63/63 [==============================] - 6s 89ms/step - loss: 0.6279 - accuracy: 0.9450 - val_loss: 0.5620 - val_accuracy: 0.9678
    Epoch 20/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.6987 - accuracy: 0.9495 - val_loss: 0.4631 - val_accuracy: 0.9653
    

Seems like our new model is clearly the best one! It consistently has validation accuracy in the **95-96** range. This is much better than the model3_1 from earlier. Further, it seems like there is NO overfitting, as the training data accuracy is always a bit lower than that of the validation data. Let's test it out on our testing data!


```python
model4test = model4.evaluate(test_dataset)
```

    6/6 [==============================] - 1s 58ms/step - loss: 0.5551 - accuracy: 0.9635
    

NICE! We are scoring with 95% accuracy on our test_dataset! Safe to say that anything we build will not compare to the stuff we can find online for now! But it was still a cool process to learn with you! 


```python

```
