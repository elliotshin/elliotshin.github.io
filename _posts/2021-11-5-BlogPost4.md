---
layout: post
title: Blog Post 4
---
# Introduction 
Hello! In today's blog post we will be going over some really complicated and tricky math that is hard to explain in detail, but conceptually easy to ascertain what we are trying to do! 

But first, what are we trying to accomplish?

We are trying to build our own spectral clustering function, which is able to create clusters out of complex data not separated by a linear equation. 

For example, a structure such as the one below can be easily separated by a line, and thus a normal KMeans algorithm will be able to group the two!
Here is our data structure:
![data](/images/4_1.png)

And here is what happens when we run KMeans:
![kmeans](/images/4_2.png)


But what happens when our data looks like this? This is an example of a spectral clustering problem

![spectral](/images/4_3.png)

Running Kmeans yields this! 

![specral_kmeans](/images/4_4.png)

As you can see, we need to change our plan of attack!

## Part A 

First, let us load the data that got us the spectral cluster from above!
```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)


```
Now, we need to first create a similarity matrix A. Essentially what this does is give relationships between each data point by calculating the distance between them. For example, the value of A[1,2] tells us the relationship between data points X[1] and X[2]. Luckily for us, there is a function called pairwise_distances() that allows us to take the data X and get the relationship described above!

```python

from sklearn.metrics import pairwise_distances 
X.shape
#A tells us if the distance between two coordinates is less than .4
#ex: is X[0] .4 away from X[1]? If so, then 
A = pairwise_distances(X)

``` 
We now need our epsilon value, which is our threshold for determining if a specific relationship should warrant the same cluster. For the purposes of this problem, it will be initially set to .4
```python
epsilon = .4

```
Now we can iteratre through A and replace the relationships with either a 1 (if it is less than or equal to epsilon) or a 0 (if it is greater than our threshold epsilon)/
```python
A
for i in range(200):
    for j in range(200):
        if A[i,j] <= epsilon:
            A[i,j] = 1
        else:
            A[i,j] = 0
np.fill_diagonal(A,0) #diagonal of the matrix have to be 0! Point X[1] really does not have a relationsip with itself! 
A

```
There we have our similarity matrix! But what do we do with it? 

## Part B
Now that we have our similarity matrix that tells us the relationships between all the data points, we now need to determine the "binary norm cut objective" of our matrix A. Essentially this function takes in the cut term (degree of similarity between rows) and the volume (sizes of our clusters) to determine our labels for our clusters. 

First we need to determine the cut term!

$$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ 

is the *cut* of the clusters $C_0$ and $C_1$.  
```python
def cut(A,y):
    """
    y are the true clusters
    if i is in cluster 0 and j is in cluster 1 --> add to sum 
    number of nonzero entities that relate points in cluster 0 to points in cluster 1
    y --> true clusters --> gives each point X a cluster value 
    A --> are points X[i] and X[j] close?
    if so, A[i][j] == 1
    if not A[i][j] == 0
    if A[i][j] == 0, then they are not in the same cluster
    but if A[i][j] == 1 but points X[i] and X[j] are not in the same cluster as determined by y --> then point A[i][j] is a point
    that relates C0 to C1 (this should be infrequent)
    
    
    """
    #get value of A[i][j], if 1 (points are closely related):
        #check to see if y[i] == y[j]
        #if y[i] == y[j] --> they belong in same cluster, do nothing
        #if y[i] != y[j] --> they do not belong in same cluster, add to sum 
    cut_term = 0;
    for i in range(len(A)):
        for j in range(len(A)):
            if A[i][j] == 1:
                if y[i] != y[j]:
                    cut_term += A[i][j].sum()
    return cut_term

```
Let's compare the cut term for our matrix A and its labels y and our matrix A and randomly generated labels!
```python
cut(A,y)

```

```python
rand_vector = np.random.rand(200).round()
cut(A,rand_vector)

```
As we can see, our cut term is much smaller for our labels (what it was made for) as compared to random values! 
The cut term determines the degree to which the rows in matrix A our similar to each other! The less similar the better, because otherwise there would be too much overlap between our clusters! 

Now we need to determine the volumes of our two clusters! The volume just means the size, and it found by summing all the values in A with label y == 1 and all the values in A with label y == 0!

```python
def vols(A,y):
    """
    computes volume (size) of clusters
    returns two values: v0 and v1 (volumes of cluster 0 and cluster 1)
    first find degree of each row:the total number of all other rows related to row  ùëñ  through  ùê¥ 
    then use degrees to find volume of c0 
    """
    A = A.astype(int)
    v0 = A[y==0].sum()
    #sum of rows related to row i in cluster 0
    v1 = A[y==1].sum() #sum of rows related to row i in cluster 1
    return v0,v1
    
vols(A,y)

```
Now that we have the two major components of our normcut function, we can put it all together!
Keep in mind, the equation for the normcut function is: 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

```python
def normcut(A,y):
    """
    uses cut(A,y) and vols(A,y) to compute binary norm cut
    """
    cut_term = cut(A,y)
    v0,v1 = vols(A,y)
    norm_cut = cut_term * ((1/v0) + (1/v1))
    return norm_cut

```
Let's compare the results of our normcut function with the true labels and random labels once more to make sure that the true labels yield a much smaller result than the random labels

```python
normcut(A,y), normcut(A,rand_vector)

```
SUCCESS! 
Now we can move on to...more math! 

## Part C 

Hmm....are we sure we did that right? How can we be sure! Lucky for us and for our computer's computational power, there is a way to confirm that our normcut function yields what it is intended to, eventually helping us determine our labels for our clusters. 

We can now create a vector z, which is essentially our guess for the datapoints' labels. It is equivalent to 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Essentially, this vector z will help us confirm that our normcut function does its intended purpose while also offering a more efficient way of generating labels for our data. 

Our normcut function should be equivalent to:

$$\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

Our transform() function will take in A and y and compute the math as described above:

```python
def transform(A,y):
    v0,v1 = vols(A,y)
    z = y.astype(float)
    #print(y)
    z[z == 0.] = 1/v0
    z[z==1.] = -1/v1
    #print(z)
    #print(y)
    D = np.diag(A.sum(axis=1))
    v_of_ones = np.ones(200)
    print(z@D@v_of_ones) #test to see if 0
    denominator = z@D@z
    D_A_diff = D - A
    numerator = z@D_A_diff@z
    #print(numerator)
    #print(denominator)
    return (numerator/denominator) * 2

```
The 0 printed above is the about of z@D@v_ofones, which essentially confirms that the vectors we made serve as intended

Now we can compare the results of our transform() and normcut() functions! Note, our computers have a finite amount of numbers they can calculate, so we can use the np.isclose(a,b) function to determine if the difference between the two results is close enough that the differences can be explained by rounding errors.

```python
a = transform(A,y)
b = normcut(A,y)
print(np.isclose(a,b))
a,b

```
WE DID IT! Now that we have confirmed that our normcut function is the same as finding the relationship between z,D, and A, we can move on to the next part!

## Part D

First lets define two functions that we will use to optimize our process earlier! 
```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 
D = np.diag(A.sum(axis=1))
d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)

```
Now we import the minimize function from scipy.optimize in order to optimize the orth_obj function. This will generate our labels based on the optimization of the function we described earlier that proved that vector z, matrix A, and matrix D could be related to each other through clusters. 

```python

from scipy.optimize import minimize
v0,v1 = vols(A,y)
z = y.astype(float)
#print(y)
z[z == 0.] = 1/v0
z[z==1.] = -1/v1
print(len(z))

z_min= minimize(orth_obj,np.ones(len(y)))
z_min = z_min.x
```

## Part E
Now let's see if our optimized guesses did a good job at predicting our clusters! 
```python
#using z_min
plt.subplot()
plt.scatter(X[z_min < 0][:,0],X[z_min < 0][:,1], color = "green")
plt.scatter(X[z_min>0][:,0],X[z_min > 0][:,1],color = "orange")

```
![spectral_zmin](/images/4_5.png)


Not bad! Let's compare it to the true cluster labels!

```python
#using original z (True Answer)
plt.subplot()
plt.scatter(X[z < 0][:,0],X[z < 0][:,1], color = "green")
plt.scatter(X[z>0][:,0],X[z > 0][:,1],color = "orange")

```

![spectral_z](/images/4_6.png)

There is a little discrepancy, but overall it did a great job!


## Part F

The Laplacian matrix is another method we can use to put together our spectral clustering algorithm. Essentially, what we are doing is finding the eigenvalues and eigenvector that will generate our labels! It is actually the eigenvector with the second-smallest corresponindg eigenvalue because the smallest eigenvalue yields 0! We essentially are optimizing again but for the next smallest vector!

The equation for the Laplacian matrix is:

$$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$

```python
from numpy.linalg import inv
from numpy.linalg import eig
D_inv = inv(D)
D_inv @ (D - A) #our Laplace Matrix
L = D_inv @ (D - A)
L
```
Now we find the eigenvalues and eigenvectors with the eig function

```python
eig_values,eig_vectors = eig(L)

```
Now we find the second-smallest eigenvalue and its corresponding eigenvector! 

```python
eig_val_sort = np.sort(eig_values, axis = None) #sort in ascending order
second_smallest = eig_val_sort[1] #find second smallest
index, = np.where(eig_values == second_smallest) #get index of smallest in original eigenvalues
index
eig_vectors[:,index] #index BY COLUMN!!! to find eigenvector

```

Now we set our found eigenvector to our z_eig, which will be our labels! We need to reshape it first so the shape matches that of our dataset X 
```python
z_eig = eig_vectors[:,index]
print(z_eig.shape)
z_eig = z_eig.reshape(200)
z_eig.shape

```

Let's plot our data!

```python
plt.subplot()
plt.scatter(X[z_eig < 0][:,0],X[z_eig < 0][:,1], color = "green")
plt.scatter(X[z_eig>0][:,0],X[z_eig > 0][:,1],color = "orange")

```
![eigenplot](/images/4_7.png)

Wow! it looks pretty good!

## Part G

Now that we have all the pieces necessary, we can put it all in one into our spectral_clustering function!

It will do the following:

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 

```python
def spectral_clustering(X,epsilon):
    """
    This function takes in Datapoints X, and, using epsilon,
    calculates similarity matrix A
    Uses A to construct Laplacian matrix
    Computes eigenvalues and eigenvectors of Lapaclian matrix to find z, the eigenvetor for the second smallest eigenvalue
    return z, our labels for our clusters 
    """
    #constructing A using pair_wise distance function 
    A = pairwise_distances(X)
    ##test our result
    for i in range(len(A)):
        for j in range(len(A)):
            if A[i,j] <= epsilon:
                A[i,j] = 1
            else:
                A[i,j] = 0
    np.fill_diagonal(A,0)
    #calculate D, diagonal of degrees
    D = np.diag(A.sum(axis = 1))
    #create L = D.inverse(D-A)
    L = inv(D) @ (D-A)
    #get eigenvalues, eigenvectors of L
    eigenvalues,eigenvectors = eig(L)
    #find second-smallest eigenvalue through sorting
    #second_smallest= np.sort(eigenvalues, axis = None)[1]
    index, = np.where(eigenvalues ==np.sort(eigenvalues, axis = None)[1]) #get eigenvalue index of second smallest
    eigenvectors[:,index] #get eigenvector column of index found above, set to z_eig
    z_eig = eigenvectors[:,index].reshape(eigenvectors.shape[0])
    z_eig[z_eig < 0] = 0
    z_eig[z_eig > 0] = 1
    return z_eig

```