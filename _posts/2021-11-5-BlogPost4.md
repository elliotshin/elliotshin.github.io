---
layout: post
title: Blog Post 4
---
# Introduction 
Hello! In today's blog post we will be going over some really complicated and tricky math that is hard to explain in detail, but conceptually easy to ascertain what we are trying to do! 

But first, what are we trying to accomplish?

We are trying to build our own spectral clustering function, which is able to create clusters out of complex data not separated by a linear equation. 

For example, a structure such as the one below can be easily separated by a line, and thus a normal KMeans algorithm will be able to group the two!
Here is our data structure:
![data](/images/4_1.png)

And here is what happens when we run KMeans:
![kmeans](/images/4_2.png)


But what happens when our data looks like this? This is an example of a spectral clustering problem

![spectral](/images/4_3.png)

Running Kmeans yields this! 

![specral_kmeans](/images/4_4.png)

As you can see, we need to change our plan of attack!

## Part A 

First, let us load the data that got us the spectral cluster from above!
```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)


```
Now, we need to first create a similarity matrix A. Essentially what this does is give relationships between each data point by calculating the distance between them. For example, the value of A[1,2] tells us the relationship between data points X[1] and X[2]. Luckily for us, there is a function called pairwise_distances() that allows us to take the data X and get the relationship described above!

```python

from sklearn.metrics import pairwise_distances 
X.shape
#A tells us if the distance between two coordinates is less than .4
#ex: is X[0] .4 away from X[1]? If so, then 
A = pairwise_distances(X)

``` 
We now need our epsilon value, which is our threshold for determining if a specific relationship should warrant the same cluster. For the purposes of this problem, it will be initially set to .4
```python
epsilon = .4

```
Now we can iteratre through A and replace the relationships with either a 1 (if it is less than or equal to epsilon) or a 0 (if it is greater than our threshold epsilon)/
```python
A
for i in range(200):
    for j in range(200):
        if A[i,j] <= epsilon:
            A[i,j] = 1
        else:
            A[i,j] = 0
np.fill_diagonal(A,0) #diagonal of the matrix have to be 0! Point X[1] really does not have a relationsip with itself! 
A

```
There we have our similarity matrix! But what do we do with it? 

## Part B
Now that we have our similarity matrix that tells us the relationships between all the data points, we now need to determine the "binary norm cut objective" of our matrix A. Essentially this function takes in the cut term (degree of similarity between rows) and the volume (sizes of our clusters) to determine our labels for our clusters. 

First we need to determine the cut term!

$$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ 

is the *cut* of the clusters $C_0$ and $C_1$.  
```python
def cut(A,y):
    """
    y are the true clusters
    if i is in cluster 0 and j is in cluster 1 --> add to sum 
    number of nonzero entities that relate points in cluster 0 to points in cluster 1
    y --> true clusters --> gives each point X a cluster value 
    A --> are points X[i] and X[j] close?
    if so, A[i][j] == 1
    if not A[i][j] == 0
    if A[i][j] == 0, then they are not in the same cluster
    but if A[i][j] == 1 but points X[i] and X[j] are not in the same cluster as determined by y --> then point A[i][j] is a point
    that relates C0 to C1 (this should be infrequent)
    
    
    """
    #get value of A[i][j], if 1 (points are closely related):
        #check to see if y[i] == y[j]
        #if y[i] == y[j] --> they belong in same cluster, do nothing
        #if y[i] != y[j] --> they do not belong in same cluster, add to sum 
    cut_term = 0;
    for i in range(len(A)):
        for j in range(len(A)):
            if A[i][j] == 1:
                if y[i] != y[j]:
                    cut_term += A[i][j].sum()
    return cut_term

```
Let's compare the cut term for our matrix A and its labels y and our matrix A and randomly generated labels!
```python
cut(A,y)

```

```python
rand_vector = np.random.rand(200).round()
cut(A,rand_vector)

```
As we can see, our cut term is much smaller for our labels (what it was made for) as compared to random values! 
The cut term determines the degree to which the rows in matrix A our similar to each other! The less similar the better, because otherwise there would be too much overlap between our clusters! 

Now we need to determine the volumes of our two clusters! The volume just means the size, and it found by summing all the values in A with label y == 1 and all the values in A with label y == 0!

```python
def vols(A,y):
    """
    computes volume (size) of clusters
    returns two values: v0 and v1 (volumes of cluster 0 and cluster 1)
    first find degree of each row:the total number of all other rows related to row  ùëñ  through  ùê¥ 
    then use degrees to find volume of c0 
    """
    A = A.astype(int)
    v0 = A[y==0].sum()
    #sum of rows related to row i in cluster 0
    v1 = A[y==1].sum() #sum of rows related to row i in cluster 1
    return v0,v1
    
vols(A,y)

```
Now that we have the two major components of our normcut function, we can put it all together!
Keep in mind, the equation for the normcut function is: 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

```python
def normcut(A,y):
    """
    uses cut(A,y) and vols(A,y) to compute binary norm cut
    """
    cut_term = cut(A,y)
    v0,v1 = vols(A,y)
    norm_cut = cut_term * ((1/v0) + (1/v1))
    return norm_cut

```
Let's compare the results of our normcut function with the true labels and random labels once more to make sure that the true labels yield a much smaller result than the random labels

```python
normcut(A,y), normcut(A,rand_vector)

```
SUCCESS! 
Now we can move on to...more math! 

## Part C 

Hmm....are we sure we did that right? How can we be sure! Lucky for us and for our computer's computational power, there is a way to confirm that our normcut function yields what it is intended to, eventually helping us determine our labels for our clusters. 

We can now create a vector z, which is essentially our guess for the datapoints' labels. It is equivalent to 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

Essentially, this vector z will help us confirm that our normcut function does its intended purpose while also offering a more efficient way of generating labels for our data. 

Our normcut function should be equivalent to:

$$\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$